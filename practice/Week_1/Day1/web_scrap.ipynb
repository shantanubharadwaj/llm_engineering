{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY','your-key-if-not-using-env')\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"The contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://openai.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://anthropic.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated code to handle javascript enabled website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import platform\n",
    "import random\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebsiteScrapper:\n",
    "    def __init__(self, url, max_retries=2, headless=True, wait_selector=\"body\", wait_timeout=10):\n",
    "        self.url = url\n",
    "        self.__text = \"\"\n",
    "        self.__title = \"\"\n",
    "        self.headless = headless\n",
    "        self.max_retries = max_retries\n",
    "        self.wait_selector = wait_selector\n",
    "        self.wait_timeout = wait_timeout\n",
    "\n",
    "    def __log_html(self, html, filename=\"last_scraped.html\"):\n",
    "        try:\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html)\n",
    "            print(f\"[✅] Saved page HTML to {filename} for debugging.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[⚠️] Could not save page HTML: {e}\")\n",
    "\n",
    "    def parse(self):\n",
    "        # Launch stealth browser\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < self.max_retries:\n",
    "            try:\n",
    "                options = uc.ChromeOptions()\n",
    "                options.headless = self.headless  # Set to False if you want to see the browser\n",
    "                options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "                options.add_argument(\"--no-sandbox\")\n",
    "                options.add_argument(\"--disable-dev-shm-usage\")\n",
    "                options.add_argument(\"--disable-gpu\")\n",
    "                with uc.Chrome(options=options) as driver:\n",
    "                    driver.get(self.url)\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(random.uniform(1, 3))\n",
    "                    WebDriverWait(driver, self.wait_timeout).until(\n",
    "                        ec.presence_of_element_located((By.CSS_SELECTOR, self.wait_selector))\n",
    "                    )\n",
    "\n",
    "                    time.sleep(1)  # Give JS a moment more if needed\n",
    "                    page_source = driver.page_source\n",
    "                    self.__log_html(page_source)\n",
    "\n",
    "                # Detect bot protection\n",
    "                if \"enable javascript\" in page_source.lower() or \"checking your browser\" in page_source.lower():\n",
    "                    self.__title = \"Blocked by Bot Protection\"\n",
    "                    self.__text = \"This website uses advanced protection (e.g., Cloudflare). Content not accessible.\"\n",
    "                    return\n",
    "\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "                self.__title = soup.title.string if soup.title else \"No title found\"\n",
    "\n",
    "                for irrelevant in soup([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                    irrelevant.decompose()\n",
    "\n",
    "                self.__text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "                try:\n",
    "                    os.remove(\"last_scraped.html\")\n",
    "                    print(\"Cleaned up debug HTML file.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not delete debug HTML file: {e}\")\n",
    "                return  # Success\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[❌] Attempt {attempt + 1} failed: {e}\")\n",
    "                attempt += 1\n",
    "                time.sleep(2)\n",
    "\n",
    "        # All retries failed\n",
    "        self.__title = \"Failed to load\"\n",
    "        self.__text = \"Website could not be scraped after several attempts.\"\n",
    "\n",
    "    def get_text(self):\n",
    "        return self.__text\n",
    "\n",
    "    def get_title(self):\n",
    "        return self.__title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSWebsiteSummarizer:\n",
    "    def __init__(self, url, headless=True):\n",
    "        self.url = url\n",
    "        self.website_scrapper = WebsiteScrapper(url, headless=headless)\n",
    "        self.system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "                            and provides a short summary, ignoring text that might be navigation related. \\\n",
    "                            Respond in markdown.\"\n",
    "\n",
    "    @staticmethod\n",
    "    def __user_prompt_for(title, content):\n",
    "        user_prompt = f\"You are looking at a website titled {title}\"\n",
    "        user_prompt += \"The contents of this website is as follows; \\\n",
    "                        please provide a short summary of this website in markdown. \\\n",
    "                        If it includes news or announcements, then summarize that too.\\n\\n\"\n",
    "        user_prompt += content\n",
    "        return user_prompt\n",
    "\n",
    "    def __messages_for(self, title, content):\n",
    "        return [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": JSWebsiteSummarizer.__user_prompt_for(title, content)}]\n",
    "\n",
    "    def __summarize(self):\n",
    "        self.website_scrapper.parse()\n",
    "        chat_config = self.__messages_for(self.website_scrapper.get_title(), self.website_scrapper.get_text())\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=chat_config)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def display_summary(self):\n",
    "        summary = self.__summarize()\n",
    "        display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = \"https://cnn.com\"\n",
    "url2 = \"https://openai.com\"\n",
    "url3 = \"https://anthropic.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_summariser = JSWebsiteSummarizer(url=url1)\n",
    "web_summariser.display_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_summariser = JSWebsiteSummarizer(url=url3, headless=False)\n",
    "web_summariser.display_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_summariser = JSWebsiteSummarizer(url=url2, headless=False)\n",
    "web_summariser.display_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_summariser = JSWebsiteSummarizer(url=url2, headless=False)\n",
    "web_summariser.display_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.1:8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "# print(response.json())\n",
    "\n",
    "# print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Understanding the foundational concepts in large language models (LLMs) is crucial to appreciate how they work and achieve remarkable performance in various natural language processing tasks.\n",
      "\n",
      "### Neural Network\n",
      "\n",
      "A **neural network** is an interconnected group of nodes or artificial neurons that mimic biological neural networks. In machine learning, these networks are used to solve complex problems by learning from data without being explicitly programmed for a specific task. A typical neural network consists of:\n",
      "\n",
      "- **Layers**: These include the input layer (where data enters), hidden layers (which process information using weights and biases), and an output layer (which produces predictions or classifications).\n",
      "- **Nodes (Neurons)**: Each node performs computations on its inputs (weighted sum) followed by a non-linear activation function to introduce complexity.\n",
      "- **Connections**: Weights assigned to each connection between nodes, which are adjusted during training to minimize the error in predictions.\n",
      "\n",
      "### Attention\n",
      "\n",
      "**Attention** is a mechanism that allows neural networks to selectively focus on relevant parts of an input sequence while processing it. This concept was introduced as a solution for issues with Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, which struggle with handling long sequences due to their sequential nature.\n",
      "\n",
      "In the context of attention mechanisms:\n",
      "\n",
      "- **Self-Attention**: It enables the model to weigh the importance of different words in a sentence relative to each other. Each word can attend to every other word when deciding its own meaning within the larger context.\n",
      "- **Scaled Dot-Product Attention**: This is one form of computing attention where the dot-product between query and key vectors determines how much weight should be given to the corresponding value vector.\n",
      "\n",
      "### Transformer\n",
      "\n",
      "The **Transformer** model, introduced in 2017 by Vaswani et al., is a type of deep learning architecture designed specifically for handling sequential data (like text or time series), utilizing only self-attention mechanisms and not relying on recurrence. Key features include:\n",
      "\n",
      "- **Self-Attention Layers**: Multiple layers that compute attention over the input sequence to capture context.\n",
      "- **Positional Encoding**: Since transformers don't have inherent order information, positional encodings are added to embeddings to give them a sense of position in the sequence.\n",
      "- **Encoder-Decoder Architecture**: Although many transformer models only use an encoder for tasks like text generation (Auto-regressive), others employ both an encoder and decoder stack, useful for translation tasks.\n",
      "\n",
      "Transformers revolutionized NLP by outperforming RNNs on various benchmarks with fewer computational requirements due to parallelizable attention mechanisms. They've become the backbone of many advanced LLMs today because they can efficiently capture long-range dependencies in text data.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"qwen2.5:14b\"\n",
    "messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
